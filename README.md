# Multi-Agent Proposal Review System

A modern, concurrent AI-powered system for evaluating SBIR/STTR proposals using multiple specialized agent personas with template-driven behavior.

## ğŸš€ Key Features

- **Template-Based Agents**: Configurable agent behavior via markdown templates
- **Concurrent Multi-Agent Review**: Specialized AI agents run simultaneously
- **LangGraph Workflow**: Robust orchestration with built-in error handling
- **Word Document Support**: Native DOCX processing with heading structure
- **LangSmith Integration**: Full observability and tracing
- **Modular Architecture**: Easy to add new agent types
- **Structured Outputs**: Agent-specific feedback + consolidated scores

## ğŸ—ï¸ Architecture

```
src/
â”œâ”€â”€ agents/                   # Agent system
â”‚   â”œâ”€â”€ templates/           # Agent behavior templates
â”‚   â”‚   â”œâ”€â”€ tech_lead.md
â”‚   â”‚   â”œâ”€â”€ business_strategist.md
â”‚   â”‚   â”œâ”€â”€ detail_checker.md
â”‚   â”‚   â”œâ”€â”€ panel_scorer.md
â”‚   â”‚   â””â”€â”€ storyteller.md
â”‚   â”œâ”€â”€ base_agent.py       # Generic agent class
â”‚   â””â”€â”€ agent_factory.py    # Agent creation and management
â”œâ”€â”€ core/                    # Document processing
â”‚   â”œâ”€â”€ document_processor.py
â”‚   â”œâ”€â”€ file_discovery.py
â”‚   â””â”€â”€ solicitation_ingester.py
â”œâ”€â”€ workflow/                # LangGraph orchestration
â”‚   â”œâ”€â”€ review_graph.py
â”‚   â””â”€â”€ state_models.py
â”œâ”€â”€ utils/                   # Output formatting
â”‚   â””â”€â”€ output_formatters.py
â””â”€â”€ cli.py                   # Command interface
```

## ğŸ¯ Agent System

| Agent ID | Persona | Focus | Output |
|----------|---------|-------|--------|
| **tech_lead** | Eric Roulo | Technical feasibility, elegance | Technical risks, missing equations |
| **business_strategist** | Gerardo | Market, ROI, partnerships | Market gaps, pricing analysis |
| **detail_checker** | Compliance | Solicitation checklist | Criterion â†’ Found? â†’ Where â†’ Fix |
| **panel_scorer** | Scoring | Criterion-based scoring | JSON: Criterion â†’ Score â†’ Justification |
| **storyteller** | Narrative | Flow, clarity | Markdown with rewrite suggestions |

## ğŸ“ File Structure

```
proposal_grader/
â”œâ”€â”€ documents/
â”‚   â”œâ”€â”€ proposal/
â”‚   â”‚   â”œâ”€â”€ main_proposal.docx    # Main proposal (Word preferred, PDF supported)
â”‚   â”‚   â”œâ”€â”€ processed/            # Cached processed documents
â”‚   â”‚   â””â”€â”€ supporting_docs/      # Supporting PDFs and DOCXs (supports sub-folders)
â”‚   â””â”€â”€ solicitation/             # Solicitation PDFs, CSVs, and MDs (supports sub-folders)
â”‚       â”œâ”€â”€ processed/            # Cached processed documents
â”œâ”€â”€ output/                       # All outputs
â”‚   â”œâ”€â”€ feedback/
â”‚   â”‚   â”œâ”€â”€ tech_lead.md
â”‚   â”‚   â”œâ”€â”€ business_strategist.md
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ scorecard.json
â”‚   â”œâ”€â”€ summary.md
â”‚   â”œâ”€â”€ action_items.md
â”‚   â”œâ”€â”€ criteria.json
â”‚   â”œâ”€â”€ solicitation.md
â”‚   â””â”€â”€ workflow.png
â”œâ”€â”€ src/agents/templates/         # Agent behavior templates
â””â”€â”€ src/                         # Source code
```

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
# Install dependencies
poetry install

# Set OpenAI API key
export OPENAI_API_KEY="your-api-key"
```

### 2. Prepare Documents

```bash
# Create directory structure
mkdir -p documents/proposal/supporting_docs
mkdir -p documents/solicitation

# Add your documents:
# - documents/proposal/main_proposal.docx (main proposal - DOCX preferred, PDF supported)
# - documents/proposal/supporting_docs/*.pdf or *.docx (supporting docs - supports sub-folders)
# - documents/solicitation/*.pdf, *.csv, *.md (solicitation docs - supports sub-folders)
```

### 3. Ingest Solicitation

```bash
# Extract evaluation criteria from solicitation
poetry run ingest-solicitation
```

### 4. Run Review

```bash
# Run with all agents (default)
poetry run review

# Run with specific agents
poetry run review --agents tech_lead,business_strategist,panel_scorer

# Skip document processing (use cached processed documents)
poetry run review --no-process-docs

# List available agents
poetry run list-agents

# Visualize workflow structure
poetry run visualize-workflow
```

## ğŸ“„ Supported File Formats

### Proposal Documents
- **Main Proposal**: DOCX (preferred for heading structure) or PDF
- **Supporting Documents**: PDF or DOCX

### Solicitation Documents  
- **PDF**: Main solicitation documents, Q&A addenda, technical specifications
- **CSV**: Evaluation criteria tables, scoring rubrics, structured data
- **MD**: Technical descriptions, FAQs, guidance documents

### Folder Organization
- All document types support sub-folder organization
- Files are discovered recursively in all sub-directories
- No depth limit on sub-folders
- Maintains original folder structure in processing

## ğŸ“Š Outputs

All outputs are saved to the `output/` folder:

### Agent-Specific Feedback
- `output/feedback/tech_lead.md` - Technical feasibility review
- `output/feedback/business_strategist.md` - Business strategy review
- `output/feedback/detail_checker.md` - Compliance checklist
- `output/feedback/panel_scorer.md` - Criterion scores
- `output/feedback/storyteller.md` - Narrative feedback

### Consolidated Results
- `output/scorecard.json` - All scores (1.0-4.0 scale)
- `output/summary.md` - Executive summary
- `output/action_items.md` - Prioritized action items

### Solicitation Processing
- `output/criteria.json` - Extracted evaluation criteria
- `output/solicitation.md` - Solicitation markdown

### Workflow Visualization
- `output/workflow.png` - Workflow diagram

## ğŸ”§ Configuration

### Environment Variables
```bash
export OPENAI_API_KEY="your-api-key"
export OPENAI_MODEL="gpt-4o"  # Optional, defaults to gpt-4o
```

### Agent Configuration
```bash
# Run with specific agents
poetry run review --agents tech_lead,business_strategist

# Custom paths and agents
poetry run review \
  --proposal-dir custom/proposal \
  --agents tech_lead,panel_scorer \
  --output-dir custom/output
```

## ğŸ¨ Agent Templates

Agent behavior is defined by templates in `src/agents/templates/`:

### Template Structure
Each agent template contains:
- **Agent Identity**: Name, role, focus, hates
- **Expertise Areas**: Key areas of expertise
- **Critical Focus Areas**: What to be critical of
- **Output Format**: How to structure feedback
- **Scoring Criteria**: How to score (1.0-4.0 scale)
- **Review Style**: Tone and approach

### Customizing Agents
To modify agent behavior:
1. Edit the template file in `src/agents/templates/<agent_id>.md`
2. Restart the review process
3. The new template will be used automatically

### Adding New Agents
1. Create new template file: `src/agents/templates/new_agent.md`
2. Follow the template structure above
3. Use the agent: `poetry run review --agents new_agent`

## ğŸ§ª Testing

Run the system test:
```bash
python test_system.py
```

## ğŸ“Š Workflow Visualization

Generate visual diagrams of the LangGraph workflow:

```bash
# Visualize default workflow
poetry run visualize-workflow

# Visualize with specific agents
poetry run visualize-workflow --agents tech_lead,business_strategist

# Custom output directory
poetry run visualize-workflow --output-dir workflow_diagrams
```

This generates:
- `workflow.png` - PNG diagram of the workflow
- `workflow.svg` - SVG diagram (better quality)

The visualization shows:
- Document processing as the initial step
- Agent nodes and their connections
- Concurrent execution paths
- Data flow through the system
- State transitions

## ğŸ”„ Iterative Workflow

1. **Prepare**: Add proposal and supporting documents
2. **Ingest**: Extract criteria from solicitation
3. **Review**: Run multi-agent review
4. **Analyze**: Check scores and action items
5. **Iterate**: Edit proposal based on feedback
6. **Repeat**: Re-run review until scores improve

## ğŸ› ï¸ Development

### Adding New Agents

1. Create new template file in `src/agents/templates/`
2. Follow the template structure
3. Use the agent via CLI: `poetry run review --agents new_agent`

### Custom Scoring

Modify the scoring criteria in agent templates or override `extract_scores_from_feedback()` in `BaseAgent`.

## ğŸ“ˆ Performance

- **Concurrent Execution**: All agents run simultaneously
- **Template-Driven**: Agent behavior defined in markdown files
- **Error Recovery**: LangGraph handles agent failures gracefully
- **LangSmith Tracing**: Full observability of agent interactions

## ğŸ” Monitoring

The system provides:
- Real-time progress indicators
- Agent completion status
- LangSmith tracing and observability
- Error messages with context
- Performance metrics

## ğŸ¯ Scoring Scale

| Score | Meaning |
|-------|---------|
| 4.0 | Outstanding, no significant weaknesses |
| 3.5 | Very strong, minor issues |
| 3.0 | Strong, some moderate issues |
| 2.5 | Adequate, several issues limit impact |
| 2.0 | Weak, major gaps |
| 1.5 | Very weak |
| 1.0 | Not responsive or fails criterion |

## ğŸ“ Migration from Legacy System

The new system replaces the old single-role rubric-based system:

- **Old**: Sequential processing with LangChain
- **New**: Concurrent processing with LangGraph
- **Old**: Hard-coded role classes
- **New**: Template-driven agent system
- **Old**: CSV output only
- **New**: Markdown + JSON outputs
- **Old**: PDF-only processing
- **New**: DOCX + PDF processing
- **Old**: No observability
- **New**: LangSmith integration

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch
3. Add tests for new functionality
4. Submit pull request

## ğŸ“„ License

MIT License - see LICENSE file for details. 